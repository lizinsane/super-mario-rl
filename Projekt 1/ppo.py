"""
Proximal Policy Optimization (PPO) f√ºr Super Mario Bros

TRAINING OUTPUT ERKL√ÑRUNG:
==========================

üéÆ ENVIRONMENT INFO (w√§hrend des Rollouts):
--------------------------------------------
Wenn ein Environment fertig ist, wird ausgegeben:
"Env {i} done. Resetting. (info: {...})"

Die info-Dictionary enth√§lt folgende Werte:

‚Ä¢ coins:         Anzahl gesammelter M√ºnzen in dieser Episode (0-100+)
                 Mehr M√ºnzen = bessere Exploration und Punktzahl

‚Ä¢ flag_get:      Hat Mario die Zielflagge erreicht? (True/False)
                 True = Level erfolgreich abgeschlossen! üéØ
                 False = Mario ist gestorben oder Zeit abgelaufen

‚Ä¢ life:          Verbleibende Leben (normalerweise 2, da Spiel mit 3 startet)
                 Bei 0 = Game Over

‚Ä¢ score:         Aktuelle Punktzahl im Spiel (0-999999)
                 Punkte durch: M√ºnzen, Gegner besiegen, Power-Ups, Zielflagge
                 H√∂here Position an Flagge = mehr Bonuspunkte

‚Ä¢ stage:         Aktueller Level/Stage (1-4 f√ºr World 1)
                 1 = Level 1-1, 2 = Level 1-2, etc.
                 Stage > 1 bedeutet Mario hat ein Level geschafft!

‚Ä¢ status:        Marios aktueller Status-String
                 "small" = Kleiner Mario (1 Hit = Tod)
                 "tall"  = Gro√üer Mario (1 Hit = wird klein)
                 "fireball" = Feuer-Mario (kann Feuerb√§lle werfen)

‚Ä¢ time:          Verbleibende Zeit auf der Level-Uhr (0-400)
                 Startet bei ~400, z√§hlt runter
                 Bei 0 = Mario stirbt (Time Over)

‚Ä¢ world:         Aktuelle Welt (normalerweise 1 f√ºr World 1)
                 SuperMarioBros-v0 spielt nur World 1

‚Ä¢ x_pos:         Horizontale Position von Mario in Pixeln (0-3000+)
                 H√∂here Werte = weiter rechts im Level
                 Level-Ende bei ca. 3161 Pixel
                 Wichtig zur Messung des Fortschritts!

‚Ä¢ y_pos:         Vertikale Position von Mario in Pixeln
                 ~79 = auf dem Boden
                 <79 = in der Luft (springt)
                 >79 = in einer Grube (meist Tod)

‚Ä¢ terminal_observation (done): 
                 Boolean-Wert, ob die Episode beendet ist (True/False)
                 Episode endet wenn:
                 - Mario stirbt (Gegner-Kontakt, Grube, Zeit abgelaufen)
                 - Level erfolgreich abgeschlossen (Flagge erreicht)
                 - Game Over (keine Leben mehr)
                 
                 True = Episode beendet ‚Üí Environment wird automatisch resettet
                 False = Episode l√§uft weiter
                 
                 Wichtig: Bei done=True werden die finalen Werte (x_pos, score, etc.)
                 als letzte Observation zur√ºckgegeben, bevor Reset erfolgt


üìä TRAINING UPDATES (alle paar Schritte):
------------------------------------------
Format: "Update {n}: avg return = {x} max_stage={y}"

‚Ä¢ Update:        Anzahl der durchgef√ºhrten Training-Updates
                 1 Update = 1 Rollout (128 Steps √ó 8 Environments) + 4 Epochen Training
                 
‚Ä¢ avg return:    Durchschnittliche Gesamtbelohnung √ºber alle 8 Environments
                 im aktuellen Rollout
                 - Negativ (-500 bis 0): Mario kommt nicht weit, stirbt fr√ºh
                 - 0-200: Lernt Grundlagen, √ºberlebt l√§nger
                 - 200-500: Macht guten Fortschritt, vermeidet Gefahren
                 - >500: Sehr gute Performance, erreicht sp√§te Level-Abschnitte
                 - >1000: Excellent! Nahe am Level-Ende
                 
‚Ä¢ max_stage:     H√∂chster erreichter Stage in diesem Rollout
                 1 = Noch in 1-1
                 2 = Hat Level 1-1 geschafft! üéâ


üèÜ EVALUATION (alle 10 Updates):
---------------------------------
Format: "[Eval] Update {n}: avg return = {x} info: {...}"

‚Ä¢ avg return:    Durchschnittliche Belohnung √ºber Evaluations-Episoden
                 (greedy policy, keine Exploration)
                 Zeigt die "echte" Performance des Agenten
                 
‚Ä¢ info:          Detaillierte Info der letzten Evaluation-Episode
                 (siehe "ENVIRONMENT INFO" oben)
                 + action_count: Counter der verwendeten Aktionen
                   Zeigt welche Aktionen wie oft ausgef√ºhrt wurden
                   Hilfreich um zu sehen ob Agent diverse Aktionen nutzt

‚Ä¢ eval_max_stage: H√∂chster erreichter Stage w√§hrend Evaluation
                  Wenn >1: Training stoppt automatisch (Level geschafft!)


ÔøΩ BEISPIEL-OUTPUT ERKL√ÑRT:
----------------------------
"Update 10: avg return = 12.28 max_stage=1"

‚Ä¢ Update 10:     10. Training-Update seit Start
                 = 10 √ó (128 Steps √ó 8 Envs) = 10.240 Spielschritte erlebt
                 Ein Update = 1 Rollout sammeln + 4 Epochen trainieren
                 
‚Ä¢ avg return = 12.28:
                 Durchschnittliche Belohnung √ºber alle 8 Environments
                 
                 INTERPRETATION DER WERTE:
                 < 0:        Mario stirbt sofort, sehr schlecht
                 0-50:       Fr√ºhe Lernphase, lernt Grundlagen ‚Üê 12.28 ist hier!
                 50-200:     Macht Fortschritte, vermeidet Gegner besser
                 200-500:    Gute Performance, kommt weit im Level
                 500-1000:   Sehr gut, erreicht sp√§te Abschnitte
                 1000+:      Exzellent, nahe am Levelende
                 
                 Bei 12.28 bedeutet das:
                 ‚úÖ Mario √ºberlebt l√§nger als zu Beginn
                 ‚úÖ Bewegt sich vorw√§rts (positiver Score!)
                 ‚úÖ Lernt grundlegende Steuerung
                 ‚ö†Ô∏è Stirbt aber noch oft fr√ºh
                 ‚ö†Ô∏è Keine komplexen Strategien
                 
‚Ä¢ max_stage=1:   H√∂chster erreichter Stage in diesem Rollout
                 Stage 1 = Level 1-1 (noch nicht abgeschlossen)
                 Stage 2 = Level 1-2 (Mario hat 1-1 geschafft! üéâ)
                 
                 Bei max_stage=1:
                 ‚ùå Level noch nicht geschafft
                 üéØ Training l√§uft weiter
                 üí° Ziel: max_stage=2 erreichen!

TYPISCHER LERNVERLAUF:
Update 1-20:     avg return -50 bis 50    ‚Üí Lernt Basics
Update 20-50:    avg return 50 bis 150    ‚Üí Erste Erfolge  
Update 50-100:   avg return 150 bis 400   ‚Üí Gute Fortschritte
Update 100-200:  avg return 400 bis 800   ‚Üí Wird richtig gut
Update 200+:     avg return 800+, stage=2 ‚Üí Schafft Level! üéâ


ÔøΩüí° TRAINING-VERLAUF INTERPRETIEREN:
------------------------------------
Gutes Zeichen:
  ‚úÖ avg return steigt kontinuierlich
  ‚úÖ x_pos Werte werden gr√∂√üer (Mario kommt weiter)
  ‚úÖ max_stage erreicht 2 (Level geschafft!)
  ‚úÖ flag_get = True in info

Warnsignal:
  ‚ö†Ô∏è  avg return bleibt konstant negativ
  ‚ö†Ô∏è  x_pos stagniert bei niedrigen Werten
  ‚ö†Ô∏è  time l√§uft oft auf 0 (zu langsam)
  ‚ö†Ô∏è  life = 0 sehr h√§ufig (stirbt zu oft)

Erfolg:
  üéâ eval_max_stage > 1 ‚Üí Training stoppt, Level geschafft!
  üéâ Model wird als "mario_1_1_clear.pt" gespeichert


‚öôÔ∏è  WICHTIGE TRAININGS-PARAMETER:
==================================

ALGORITHMUS-PARAMETER (PPO-spezifisch):
----------------------------------------
‚Ä¢ lr (Learning Rate):         2.5e-4 (0.00025)
                              Schrittgr√∂√üe f√ºr Gewichtsaktualisierungen
                              Zu hoch: Instabiles Training, oszilliert
                              Zu niedrig: Langsame Konvergenz
                              2.5e-4 ist Standard f√ºr PPO

‚Ä¢ rollout_steps:              128
                              Anzahl der Steps pro Environment vor einem Update
                              128 Steps √ó 8 Envs = 1024 Samples pro Rollout
                              Mehr = stabilere Gradienten, aber l√§nger bis Update
                              
‚Ä¢ epochs:                     4
                              Wie oft dieselben Daten f√ºr Training verwendet werden
                              PPO nutzt Daten mehrfach (anders als Policy Gradient)
                              Zu viel: √úberanpassung, Policy wird zu gierig
                              4 ist typischer PPO-Wert

‚Ä¢ minibatch_size:             64
                              Batch-Gr√∂√üe f√ºr jedes Gradient-Update
                              Aus 1024 Samples werden 16 Minibatches √° 64 gebildet
                              Gr√∂√üer = stabilere Gradienten, mehr GPU-Speicher
                              Kleiner = mehr Updates, weniger Speicher

‚Ä¢ clip_eps (Œµ):               0.2
                              PPO Clipping-Parameter (kritisch f√ºr PPO!)
                              Begrenzt wie stark die Policy sich √§ndern darf
                              Policy Ratio wird auf [0.8, 1.2] geclipped
                              Verhindert zu gro√üe Policy-Updates
                              0.2 = Standard, 0.1-0.3 sind √ºblich

‚Ä¢ vf_coef (Value Function):   0.5
                              Gewichtung des Value-Loss in der Gesamt-Loss
                              Total Loss = Policy Loss + 0.5 √ó Value Loss - 0.01 √ó Entropy
                              H√∂her = Value-Function wird genauer, Policy langsamer

‚Ä¢ ent_coef (Entropy):         0.01
                              Gewichtung der Entropy-Bonus (f√∂rdert Exploration)
                              H√∂her = mehr Exploration, mehr Zuf√§lligkeit
                              Zu hoch: Agent bleibt zu zuf√§llig
                              Zu niedrig: Agent wird zu schnell gierig (greedy)

‚Ä¢ gamma (Discount):           0.99
                              Wie stark zuk√ºnftige Belohnungen gewichtet werden
                              0.99 = 99% der zuk√ºnftigen Belohnung z√§hlt
                              H√∂her = weitsichtiger, plant langfristig
                              
‚Ä¢ lambda (Œª, GAE):            0.95
                              GAE (Generalized Advantage Estimation) Parameter
                              Trade-off zwischen Bias und Varianz
                              0.95 = Standard, balanciert Genauigkeit und Stabilit√§t


ENVIRONMENT-PARAMETER:
----------------------
‚Ä¢ num_env:                    8
                              Anzahl parallel laufender Environments
                              Mehr = schnellere Datensammlung, bessere GPU-Nutzung
                              Begrenzt durch RAM und GPU-Speicher
                              8 ist guter Kompromiss f√ºr Consumer-Hardware

‚Ä¢ obs_dim (n_frame):          4
                              Anzahl gestapelter Frames als Observation
                              4 aufeinanderfolgende Frames ‚Üí Agent sieht Bewegung
                              Wichtig da ein Frame allein keine Geschwindigkeit zeigt

‚Ä¢ act_dim:                    12 (COMPLEX_MOVEMENT)
                              Anzahl m√∂glicher Aktionen
                              12 = Kombinationen von: rechts, links, springen, etc.


SPEICHER- & EVALUATIONS-PARAMETER:
-----------------------------------
‚Ä¢ Save Interval:              Alle 50 Updates ‚Üí "mario_1_1_ppo.pt"
                              Regelm√§√üige Checkpoints f√ºr Fortschritt
                              
‚Ä¢ Eval Interval:              Alle 10 Updates
                              Testet Agent ohne Exploration (greedy policy)
                              Zeigt echte Performance
                              
‚Ä¢ Success Criterion:          eval_max_stage > 1
                              Stoppt Training wenn Level geschafft
                              Speichert finales Model als "mario_1_1_clear.pt"


NETZWERK-ARCHITEKTUR (ActorCritic):
------------------------------------
‚Ä¢ Conv Layer 1:               4 ‚Üí 32 Filter, Kernel 8√ó8, Stride 4
                              Extrahiert grobe Features aus Frames
                              
‚Ä¢ Conv Layer 2:               32 ‚Üí 64 Filter, Kernel 3√ó3, Stride 1
                              Verfeinert Features
                              
‚Ä¢ Linear Layer:               20736 ‚Üí 512 Neuronen
                              Fully-Connected Layer nach Flatten
                              
‚Ä¢ Policy Head:                512 ‚Üí 12 (Aktionen)
                              Gibt Wahrscheinlichkeit f√ºr jede Aktion
                              
‚Ä¢ Value Head:                 512 ‚Üí 1 (State Value)
                              Sch√§tzt Wert des aktuellen Zustands


üí° PARAMETER-TUNING TIPPS:
---------------------------
F√ºr schnelleres Training:
  ‚Üí Erh√∂he num_env (z.B. 16, wenn genug RAM/GPU)
  ‚Üí Erh√∂he rollout_steps (z.B. 256)
  
F√ºr stabileres Training:
  ‚Üí Reduziere lr auf 1e-4
  ‚Üí Reduziere clip_eps auf 0.1
  
F√ºr mehr Exploration:
  ‚Üí Erh√∂he ent_coef auf 0.02-0.05
  
Bei √úberanpassung:
  ‚Üí Reduziere epochs auf 3
  ‚Üí Erh√∂he Entropy Bonus
"""

from collections import Counter
import csv
import os
from datetime import datetime

import gym_super_mario_bros
import gym as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from gym_super_mario_bros.actions import COMPLEX_MOVEMENT
from nes_py.wrappers import JoypadSpace

from wrappers import *

device = "cpu"
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"


def init_csv_logger(filename="training_log.csv"):
    """
    Initialisiert CSV-Datei f√ºr Training-Logs.
    Wenn Datei existiert, wird weiter angeh√§ngt.
    """
    file_exists = os.path.exists(filename)
    
    if not file_exists:
        # Erstelle neue CSV mit Header
        with open(filename, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                'timestamp',
                'update',
                'avg_return',
                'max_stage',
                'eval_avg_return',
                'eval_max_stage',
                'eval_coins',
                'eval_flag_get',
                'eval_life',
                'eval_score',
                'eval_status',
                'eval_time',
                'eval_x_pos',
                'eval_y_pos',
                'device'
            ])
        print(f"üìä CSV-Logger initialisiert: {filename}")
    else:
        print(f"üìä CSV-Logger wird fortgesetzt: {filename}")
    
    return filename


def log_to_csv(filename, update, avg_return, max_stage, eval_data=None):
    """
    Schreibt Training-Daten in CSV-Datei.
    
    Args:
        filename: CSV-Dateiname
        update: Update-Nummer
        avg_return: Durchschnittliche Belohnung
        max_stage: H√∂chster erreichter Stage
        eval_data: Optional - Dictionary mit Evaluations-Daten (avg_score, info, eval_max_stage)
    """
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # Basis-Daten (bei jedem Update)
    row = [
        timestamp,
        update,
        f"{avg_return:.2f}",
        max_stage,
    ]
    
    # Evaluations-Daten (nur alle 10 Updates)
    if eval_data:
        info = eval_data.get('info', {})
        row.extend([
            f"{eval_data.get('avg_score', ''):.2f}" if eval_data.get('avg_score') else '',
            eval_data.get('eval_max_stage', ''),
            info.get('coins', ''),
            info.get('flag_get', ''),
            info.get('life', ''),
            info.get('score', ''),
            info.get('status', ''),
            info.get('time', ''),
            info.get('x_pos', ''),
            info.get('y_pos', ''),
        ])
    else:
        # Leere Felder wenn keine Evaluation
        row.extend([''] * 10)
    
    row.append(device)
    
    # Schreibe in CSV
    with open(filename, 'a', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(row)


def make_env():
    env = gym_super_mario_bros.make("SuperMarioBros-v0")
    env = JoypadSpace(env, COMPLEX_MOVEMENT)
    env = wrap_mario(env)
    return env


def get_reward(r):
    r = np.sign(r) * (np.sqrt(abs(r) + 1) - 1) + 0.001 * r
    return r


class ActorCritic(nn.Module):
    def __init__(self, n_frame, act_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(n_frame, 32, 8, 4),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, 1),
            nn.ReLU(),
        )
        self.linear = nn.Linear(20736, 512)
        self.policy_head = nn.Linear(512, act_dim)
        self.value_head = nn.Linear(512, 1)

    def forward(self, x):
        if x.dim() == 4:
            x = x.permute(0, 3, 1, 2)
        elif x.dim() == 3:
            x = x.permute(2, 0, 1)
        x = self.net(x)
        x = x.reshape(-1, 20736)
        x = torch.relu(self.linear(x))

        return self.policy_head(x), self.value_head(x).squeeze(-1)

    def act(self, obs):
        logits, value = self.forward(obs)
        dist = torch.distributions.Categorical(logits=logits)
        action = dist.sample()
        logprob = dist.log_prob(action)
        return action, logprob, value


def compute_gae_batch(rewards, values, dones, gamma=0.99, lam=0.95):
    T, N = rewards.shape
    advantages = torch.zeros_like(rewards)
    gae = torch.zeros(N, device=device)

    for t in reversed(range(T)):
        not_done = 1.0 - dones[t]
        delta = rewards[t] + gamma * values[t + 1] * not_done - values[t]
        gae = delta + gamma * lam * not_done * gae
        advantages[t] = gae

    returns = advantages + values[:-1]
    return advantages, returns


def rollout_with_bootstrap(envs, model, rollout_steps, init_obs):
    obs = init_obs
    obs = torch.tensor(obs, dtype=torch.float32).to(device)
    obs_buf, act_buf, rew_buf, done_buf, val_buf, logp_buf = [], [], [], [], [], []

    for _ in range(rollout_steps):
        obs_buf.append(obs)

        with torch.no_grad():
            action, logp, value = model.act(obs)

        val_buf.append(value)
        logp_buf.append(logp)
        act_buf.append(action)

        actions = action.cpu().numpy()
        next_obs, reward, done, infos = envs.step(actions)

        reward = [get_reward(r) for r in reward]
        # done = np.logical_or(terminated)

        rew_buf.append(torch.tensor(reward, dtype=torch.float32).to(device))
        done_buf.append(torch.tensor(done, dtype=torch.float32).to(device))

        # AsyncVectorEnv resettet automatisch! Kein manueller Reset n√∂tig
        # Wenn done=True, enth√§lt next_obs bereits die neue Episode
        for i, d in enumerate(done):
            if d:
                print(f"Env {i} done. Resetting. (info: {infos[i]})")
                # next_obs[i] ist bereits der Reset-State von AsyncVectorEnv

        obs = torch.tensor(next_obs, dtype=torch.float32).to(device)
        max_stage = max([i["stage"] for i in infos])

    with torch.no_grad():
        _, last_value = model.forward(obs)

    obs_buf = torch.stack(obs_buf)
    act_buf = torch.stack(act_buf)
    rew_buf = torch.stack(rew_buf)
    done_buf = torch.stack(done_buf)
    val_buf = torch.stack(val_buf)
    val_buf = torch.cat([val_buf, last_value.unsqueeze(0)], dim=0)
    logp_buf = torch.stack(logp_buf)

    adv_buf, ret_buf = compute_gae_batch(rew_buf, val_buf, done_buf)
    adv_buf = (adv_buf - adv_buf.mean()) / (adv_buf.std() + 1e-8)

    return {
        "obs": obs_buf,  # [T, N, obs_dim]
        "actions": act_buf,
        "logprobs": logp_buf,
        "advantages": adv_buf,
        "returns": ret_buf,
        "max_stage": max_stage,
        "last_obs": obs,
    }


def evaluate_policy(env, model, episodes=5, render=False):
    """
    Function to evaluate the learned policy

    Args:
    env: gym.Env single environment (not vector!)

    model: ActorCritic model

    episodes: number of episodes to evaluate

    render: whether to visualize (if True, display on screen)

    Returns:
    avg_return: average total reward
    """
    model.eval()
    total_returns = []
    actions = []
    stages = []
    for ep in range(episodes):
        obs = env.reset()
        done = False
        total_reward = 0
        if render:
            env.render()
        while not done:
            obs_tensor = (
                torch.tensor(np.array(obs), dtype=torch.float32).unsqueeze(0).to(device)
            )
            with torch.no_grad():
                logits, _ = model(obs_tensor)
                dist = torch.distributions.Categorical(logits=logits)
                action = dist.probs.argmax(dim=-1).item()  # greedy action
                actions.append(action)

            obs, reward, done, info = env.step(action)
            stages.append(info["stage"])
            total_reward += reward

        total_returns.append(total_reward)
        info["action_count"] = Counter(actions)
    model.train()
    return np.mean(total_returns), info, max(stages)


def train_ppo():
    num_env = 8
    # WICHTIG: AsyncVectorEnv f√ºr echte Parallelisierung auf Apple Silicon!
    # Nutzt mehrere CPU-Cores gleichzeitig ‚Üí 3-5x schneller
    # F√ºr Mac mit M1/M2/M3 sehr empfohlen!
    try:
        envs = gym.vector.AsyncVectorEnv([lambda: make_env() for _ in range(num_env)])
        print(f"‚úÖ Nutze AsyncVectorEnv: {num_env} Environments laufen parallel auf mehreren CPU-Cores")
    except:
        # Fallback auf SyncVectorEnv falls AsyncVectorEnv Probleme macht
        envs = gym.vector.SyncVectorEnv([lambda: make_env() for _ in range(num_env)])
        print(f"‚ö†Ô∏è  Nutze SyncVectorEnv: {num_env} Environments laufen sequenziell (langsamer)")
    
    obs_dim = envs.single_observation_space.shape[-1]
    act_dim = envs.single_action_space.n
    print(f"{obs_dim=} {act_dim=}")
    model = ActorCritic(obs_dim, act_dim).to(device)
    
    # Lade vortrainiertes Modell falls vorhanden, sonst starte von Null
    checkpoint_file = "mario_1_1_ppo.pt"
    start_update = 0
    
    try:
        # PyTorch 2.6+ ben√∂tigt weights_only=False f√ºr vollst√§ndige Checkpoints
        checkpoint = torch.load(checkpoint_file, weights_only=False)
        
        # Falls alter Checkpoint-Format (nur model state_dict)
        if isinstance(checkpoint, dict) and 'model_state_dict' not in checkpoint:
            model.load_state_dict(checkpoint)
            print(f"‚úÖ Altes Modell geladen: {checkpoint_file}")
            print(f"‚ÑπÔ∏è  Optimizer-State nicht verf√ºgbar (alter Checkpoint)")
        # Neues Checkpoint-Format (vollst√§ndig)
        elif isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            start_update = checkpoint.get('update', 0)
            print(f"‚úÖ Vollst√§ndiger Checkpoint geladen: {checkpoint_file}")
            print(f"üìä Fortsetzen ab Update {start_update}")
        else:
            model.load_state_dict(checkpoint)
            print(f"‚úÖ Modell geladen: {checkpoint_file}")
            
    except FileNotFoundError:
        print(f"‚ÑπÔ∏è  Kein vortrainiertes Modell gefunden. Starte Training von Null.")
    
    optimizer = optim.Adam(model.parameters(), lr=2.5e-4)
    
    # Lade Optimizer-State falls vorhanden (nur bei neuem Checkpoint-Format)
    try:
        checkpoint = torch.load(checkpoint_file)
        if isinstance(checkpoint, dict) and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print(f"‚úÖ Optimizer-State geladen (Momentum erhalten)")
    except:
        pass

    rollout_steps = 128
    epochs = 4
    minibatch_size = 64
    clip_eps = 0.2
    vf_coef = 0.5
    ent_coef = 0.01
    eval_env = make_env()
    eval_env.reset()
    
    # Initialisiere CSV-Logger
    csv_filename = init_csv_logger("training_log.csv")

    init_obs = envs.reset()
    update = start_update  # Starte bei gespeichertem Update-Z√§hler
    while True:
        update += 1
        batch = rollout_with_bootstrap(envs, model, rollout_steps, init_obs)
        init_obs = batch["last_obs"]

        T, N = rollout_steps, envs.num_envs
        total_size = T * N

        obs = batch["obs"].reshape(total_size, *envs.single_observation_space.shape)
        act = batch["actions"].reshape(total_size)
        logp_old = batch["logprobs"].reshape(total_size)
        adv = batch["advantages"].reshape(total_size)
        ret = batch["returns"].reshape(total_size)

        for _ in range(epochs):
            idx = torch.randperm(total_size)
            for start in range(0, total_size, minibatch_size):
                i = idx[start : start + minibatch_size]
                logits, value = model(obs[i])
                dist = torch.distributions.Categorical(logits=logits)
                logp = dist.log_prob(act[i])
                ratio = torch.exp(logp - logp_old[i])
                clipped = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * adv[i]
                policy_loss = -torch.min(ratio * adv[i], clipped).mean()
                value_loss = (ret[i] - value).pow(2).mean()
                entropy = dist.entropy().mean()
                loss = policy_loss + vf_coef * value_loss - ent_coef * entropy

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        # logging
        avg_return = batch["returns"].mean().item()
        max_stage = batch["max_stage"]
        print(f"Update {update}: avg return = {avg_return:.2f} {max_stage=}")
        
        # Evaluiere bei jedem Update
        avg_score, info, eval_max_stage = evaluate_policy(
            eval_env, model, episodes=1, render=False
        )
        
        # Logge alle Daten in CSV (jedes Update)
        eval_data = {
            'avg_score': avg_score,
            'info': info,
            'eval_max_stage': eval_max_stage
        }
        log_to_csv(csv_filename, update, avg_return, max_stage, eval_data)
        
        # Zeige Evaluations-Ergebnisse nur bei jedem 10. Update an
        if update % 10 == 0:
            print(f"[Eval] Update {update}: avg return = {avg_score:.2f} info: {info}")
            
            if eval_max_stage > 1:
                # Erfolg! Speichere finales Modell
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'update': update,
                    'avg_score': avg_score,
                    'success': True
                }, "mario_1_1_clear.pt")
                print(f"üéâ Level geschafft! Finales Modell gespeichert: mario_1_1_clear.pt")
                break
        if update > 0 and update % 50 == 0:
            # Checkpoint: Speichere ALLES f√ºr sp√§teres Fortsetzen
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'update': update,
                'avg_return': avg_return,
                'max_stage': max_stage
            }, "mario_1_1_ppo.pt")
            print(f"üíæ Checkpoint gespeichert bei Update {update}")


if __name__ == "__main__":
    train_ppo()